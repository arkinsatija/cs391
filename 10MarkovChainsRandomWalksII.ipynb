{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chains and Random Walks -- Convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import slideUtilities as sl\n",
    "import laUtilities as ut\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
       "    display: None ! important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
    "    display: None ! important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "%Set up useful MathJax (Latex) macros.\n",
    "%See http://docs.mathjax.org/en/latest/tex.html#defining-tex-macros\n",
    "%These are for use in the slideshow\n",
    "$\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}$\n",
    "$\\newcommand{\\vx}{{\\mathbf x}}$\n",
    "$\\newcommand{\\hx}{\\hat{\\mathbf x}}$\n",
    "$\\newcommand{\\vbt}{{\\mathbf\\beta}}$\n",
    "$\\newcommand{\\vy}{{\\mathbf y}}$\n",
    "$\\newcommand{\\vz}{{\\mathbf z}}$\n",
    "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
    "$\\newcommand{\\vu}{{\\mathbf u}}$\n",
    "$\\newcommand{\\vv}{{\\mathbf v}}$\n",
    "$\\newcommand{\\vw}{{\\mathbf w}}$\n",
    "$\\newcommand{\\col}{{\\operatorname{Col}}}$\n",
    "$\\newcommand{\\nul}{{\\operatorname{Nul}}}$\n",
    "$\\newcommand{\\vb}{{\\mathbf b}}$\n",
    "$\\newcommand{\\va}{{\\mathbf a}}$\n",
    "$\\newcommand{\\ve}{{\\mathbf e}}$\n",
    "$\\newcommand{\\setb}{{\\mathcal{B}}}$\n",
    "$\\newcommand{\\rank}{{\\operatorname{rank}}}$\n",
    "$\\newcommand{\\vp}{{\\mathbf p}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}\n",
    "\\newcommand{\\vx}{{\\mathbf x}}\n",
    "\\newcommand{\\hx}{\\hat{\\mathbf x}}\n",
    "\\newcommand{\\vbt}{{\\mathbf\\beta}}\n",
    "\\newcommand{\\vy}{{\\mathbf y}}\n",
    "\\newcommand{\\vz}{{\\mathbf z}}\n",
    "\\newcommand{\\vb}{{\\mathbf b}}\n",
    "\\newcommand{\\vu}{{\\mathbf u}}\n",
    "\\newcommand{\\vv}{{\\mathbf v}}\n",
    "\\newcommand{\\vw}{{\\mathbf w}}\n",
    "\\newcommand{\\va}{{\\mathbf a}}\n",
    "\\newcommand{\\ve}{{\\mathbf e}}\n",
    "\\newcommand{\\vp}{{\\mathbf p}}\n",
    "\\newcommand{\\R}{{\\mathbb{R}}}\n",
    "\\newcommand{\\col}{{\\operatorname{Col}}}\n",
    "\\newcommand{\\nul}{{\\operatorname{Nul}}}\n",
    "\\newcommand{\\rank}{{\\operatorname{rank}}}\n",
    "\\newcommand{\\setb}{{\\mathcal{B}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random walks with Linear Algebra\n",
    "\n",
    "\n",
    "\n",
    "* Instead of thinking of initial state as a vector with one 1 and zero everywhere else,  we can think\n",
    "of the vector that specifies the probability of being at state $i\\in S$. Then the randomness\n",
    "goes away and this vector evolves according to deterministic rule.\n",
    "\n",
    "* Let the initial distribution be given by the row vector $\\mathbf{x}\\in \\mathbb{R}^n$ such that\n",
    "$\\sum_i \\mathbf{x}(i) = 1$\n",
    "\n",
    "After one step, the probability of being at state $i$ is:\n",
    "\n",
    "$$\\sum_j \\mathbf{x}(j)M(j,i),$$ \n",
    "\n",
    "which corresponds to a new distribution $\\mathbf{x}M$. \n",
    "\n",
    "* Can you check that $\\mathbf{x}M$ is again a distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* One can think of $\\mathbf{x}$ as describing the amount of probability fluid sitting\n",
    "at each node, such that the sum of the amounts is $1$. \n",
    "\n",
    "* After one step, the fluid sitting at node $i$ *distributes* to its neighbors, such that $M(i,j)$ fraction goes to $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Recall**:  A distribution $\\pi$ for the Markov chain $M$ is a stationary distribution if\n",
    "$\\pi M = \\pi$. \n",
    "\n",
    "\n",
    "**Question**: Does this definition remind you of anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revist the eigenvalues and eigenvectors \n",
    "\n",
    "**Eigenvalues**: Recall that if $M \\in \\mathbb{R}^{n\\times n}$\n",
    "is a square symmetric matrix of $n$ rows and $n$ columns then an eigenvalue of $M$ is a scalar $\\lambda\\in \\mathbb{R}$ such that exists a vector $\\mathbf{x}\\in \\mathbb{R}^n$\n",
    "for which $\\mathbf{x} M\\cdot  = \\lambda \\cdot \\mathbf{x}$. \n",
    "\n",
    "\n",
    "* The vector $x$ is called the *eigenvector* corresponding to the *eigenvalue* $\\lambda$. \n",
    "\n",
    "* $M$ has $n$ real eigenvalues denoted $\\lambda_1\\leq \\ldots \\leq \\lambda_n$.\n",
    "(The multiset of eigenvalues is called the spectrum.) \n",
    "\n",
    "* The eigenvectors associated with these eigenvalues form an orthogonal basis\n",
    "for the vector space in $\\mathbb{R}^n$.\n",
    "(for any two such vectors the inner product is zero and all vectors\n",
    "are linear independent). \n",
    "\n",
    "* The word eigenvector comes from German, and it means “one’s own vector. ”The eigenvectors are n prefered directions for the matrix, such that applying the matrix on these directions amounts to simple scaling by the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The eigenvalues and eigenvectors of a transition matrix\n",
    "\n",
    "\n",
    "Recall that for the stationary distribution: \n",
    "\n",
    "$$ \\pi\\; M = \\pi $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Question**: Is $1$ an eigenvalue of $M$?\n",
    "\n",
    "**Answer**: Recall that $\\lambda$ is an eigenvalue of $M$ iff $\\mathbf{v}M=\\lambda \\mathbf{v}$ for some *nonzero* vector $\\mathbf{v}$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{v}M=\\lambda \\mathbf{v}\n",
    "$$\n",
    "is equivalent to\n",
    "\n",
    "$$\n",
    "\\mathbf{v}(M-\\lambda I ) = 0.\n",
    "$$\n",
    "\n",
    "If $(M-\\lambda I)$ is invertible then $\\mathbf{v}=(M-\\lambda I)^{-1}\\cdot 0 = 0$, which is a contradiction as the vector $\\mathbf{v}$ is by definition non-zero. \n",
    "\n",
    "Therefore, $(M-\\lambda I)$ is *not invertible* which means that $\\text{det}(M-\\lambda I)=0$. Since $M$ is stochastic (and its rows sum up to $1$) then $\\lambda = 1$ is an eigenvalue as $\\text{det}(M-\\lambda I)=0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Question**: Is there an eigenvalue of $M$ that is larger than $1$?\n",
    "\n",
    "**Answer**: Now, suppose that there exists a vector $\\mathbb{x}$ such that \n",
    "$\\mathbb{x}M=\\lambda \\mathbb{x}$ for some $\\lambda >1$. Since the rows of $M$ are nonnegative and sum to $1$, each element of vector $\\mathbb{x}$ is a convex combination of the components of $\\mathbb{x}$, which can be no greater than $x_\\max$ the largest component of $\\mathbb{x}$. On the other hand, at least one element of $\\lambda\\mathbb{x}$ is greater than $x_\\max$, which proves that $\\lambda >1$ is impossible."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
